{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc080774-f41a-4503-a064-c0ac819eeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg fetch new batch of features and compute predictions and save to feature store\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd038bf-8550-420b-9ec2-308198fca80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b8ae0d-58dd-4772-8931-25ff5fccb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26612685-265e-4415-a149-5751be8f994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 00:35:38,009 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-04 00:35:38,012 INFO: Initializing external client\n",
      "2025-03-04 00:35:38,012 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-04 00:35:39,258 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215646\n",
      "Fetching data from 2025-02-03 05:35:38.009177+00:00 to 2025-03-04 04:35:38.009177+00:00\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (14.24s) \n",
      "Batch data fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.inference import get_feature_store\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd  \n",
    "import src.config as config\n",
    "\n",
    "# Get the current UTC timestamp\n",
    "current_date = pd.Timestamp.now(tz='Etc/UTC')\n",
    "feature_store = get_feature_store()\n",
    "\n",
    "# Define the time window for fetching data\n",
    "fetch_data_to = current_date - timedelta(hours=1)\n",
    "fetch_data_from = current_date - timedelta(days=29)\n",
    "print(f\"Fetching data from {fetch_data_from} to {fetch_data_to}\")\n",
    "\n",
    "# Retrieve the feature view using your config values\n",
    "feature_view = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_NAME, version=config.FEATURE_VIEW_VERSION\n",
    ")\n",
    "\n",
    "# Attempt to get batch data using Spark execution via arrow_flight_config\n",
    "ts_data = feature_view.get_batch_data(\n",
    "    start_time=(fetch_data_from - timedelta(days=1)),\n",
    "    end_time=(fetch_data_to + timedelta(days=1)),\n",
    "    read_options={\"arrow_flight_config\": {\"use_spark\": True}}\n",
    ")\n",
    "\n",
    "# Filter the data to only include the desired time window and sort\n",
    "ts_data = ts_data[ts_data.pickup_hour.between(fetch_data_from, fetch_data_to)]\n",
    "ts_data = ts_data.sort_values([\"pickup_location_id\", \"pickup_hour\"]).reset_index(drop=True)\n",
    "\n",
    "# Remove timezone information for consistency\n",
    "ts_data[\"pickup_hour\"] = ts_data[\"pickup_hour\"].dt.tz_localize(None)\n",
    "\n",
    "print(\"Batch data fetched successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22253dce-140a-4296-a48b-35c6c7655b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 00:36:37,461 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-04 00:36:37,462 INFO: Initializing external client\n",
      "2025-03-04 00:36:37,462 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-04 00:36:38,827 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215646\n",
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "from src.inference import load_model_from_registry\n",
    "\n",
    "model = load_model_from_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b868f801-03a3-4d51-8e24-88231e664f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 07:19:14,295 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-04 07:19:14,298 INFO: Initializing external client\n",
      "2025-03-04 07:19:14,298 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-04 07:19:15,040 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215646\n",
      "Fetching data from 2025-02-03 12:19:14.294951+00:00 to 2025-03-04 11:19:14.294951+00:00\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.95s) \n",
      "Batch data fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.inference import get_feature_store\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd  \n",
    "import src.config as config\n",
    "\n",
    "# Get the current UTC timestamp  \n",
    "current_date = pd.Timestamp.now(tz='Etc/UTC')\n",
    "feature_store = get_feature_store()\n",
    "\n",
    "# Define the time window for fetching data\n",
    "fetch_data_to = current_date - timedelta(hours=1)\n",
    "fetch_data_from = current_date - timedelta(days=29)\n",
    "print(f\"Fetching data from {fetch_data_from} to {fetch_data_to}\")\n",
    "\n",
    "# Retrieve the feature view using your config values\n",
    "feature_view = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_NAME, version=config.FEATURE_VIEW_VERSION\n",
    ")\n",
    "\n",
    "# Attempt to get batch data using Spark execution via arrow_flight_config\n",
    "ts_data = feature_view.get_batch_data(\n",
    "    start_time=(fetch_data_from - timedelta(days=1)),\n",
    "    end_time=(fetch_data_to + timedelta(days=1)),\n",
    "    read_options={\"arrow_flight_config\": {\"use_spark\": True, \"timeout\": 60000}}\n",
    ")\n",
    "\n",
    "# Filter the data to only include the desired time window\n",
    "ts_data = ts_data[ts_data.pickup_hour.between(fetch_data_from, fetch_data_to)]\n",
    "\n",
    "# Sort and reset index for consistency\n",
    "ts_data.sort_values(by=[\"pickup_location_id\", \"pickup_hour\"], inplace=True)\n",
    "ts_data = ts_data.reset_index(drop=True)\n",
    "\n",
    "# Remove timezone information from pickup_hour\n",
    "ts_data[\"pickup_hour\"] = ts_data[\"pickup_hour\"].dt.tz_localize(None)\n",
    "\n",
    "print(\"Batch data fetched successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "751d63ab-8e98-4087-a3cf-d79bd1bd88e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 07:35:07,503 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-04 07:35:07,507 INFO: Initializing external client\n",
      "2025-03-04 07:35:07,507 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-04 07:35:08,150 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215646\n",
      "Fetching data from 2025-02-03 12:35:07.503949+00:00 to 2025-03-04 11:35:07.503949+00:00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Reading data with Hive is not supported when using hopsworks client version >= 4.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m feature_view \u001b[38;5;241m=\u001b[39m feature_store\u001b[38;5;241m.\u001b[39mget_feature_view(\n\u001b[1;32m     21\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mFEATURE_VIEW_NAME, version\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mFEATURE_VIEW_VERSION\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Now, fetch the batch data (this should force Spark because Arrow Flight is disabled)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m ts_data \u001b[38;5;241m=\u001b[39m feature_view\u001b[38;5;241m.\u001b[39mget_batch_data(\n\u001b[1;32m     26\u001b[0m     start_time\u001b[38;5;241m=\u001b[39m(fetch_data_from \u001b[38;5;241m-\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     27\u001b[0m     end_time\u001b[38;5;241m=\u001b[39m(fetch_data_to \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     28\u001b[0m     read_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_spark\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m60000\u001b[39m}  \u001b[38;5;66;03m# timeout in ms (60 sec)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Filter to the desired time window\u001b[39;00m\n\u001b[1;32m     32\u001b[0m ts_data \u001b[38;5;241m=\u001b[39m ts_data[ts_data\u001b[38;5;241m.\u001b[39mpickup_hour\u001b[38;5;241m.\u001b[39mbetween(fetch_data_from, fetch_data_to)]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/hopsworks_common/usage.py:246\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    245\u001b[0m     exception \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/hopsworks_common/usage.py:242\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/hsfs/feature_view.py:1085\u001b[0m, in \u001b[0;36mFeatureView.get_batch_data\u001b[0;34m(self, start_time, end_time, read_options, spine, primary_key, event_time, inference_helper_columns, dataframe_type, transformed, transformation_context, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_scoring_server\u001b[38;5;241m.\u001b[39m_serving_initialized:\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_batch_scoring()\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_view_engine\u001b[38;5;241m.\u001b[39mget_batch_data(\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1087\u001b[0m     start_time,\n\u001b[1;32m   1088\u001b[0m     end_time,\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_scoring_server\u001b[38;5;241m.\u001b[39mtraining_dataset_version,\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_scoring_server\u001b[38;5;241m.\u001b[39m_model_dependent_transformation_functions,\n\u001b[1;32m   1091\u001b[0m     read_options,\n\u001b[1;32m   1092\u001b[0m     spine,\n\u001b[1;32m   1093\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m primary_key,\n\u001b[1;32m   1094\u001b[0m     event_time,\n\u001b[1;32m   1095\u001b[0m     inference_helper_columns,\n\u001b[1;32m   1096\u001b[0m     dataframe_type,\n\u001b[1;32m   1097\u001b[0m     transformed\u001b[38;5;241m=\u001b[39mtransformed,\n\u001b[1;32m   1098\u001b[0m     transformation_context\u001b[38;5;241m=\u001b[39mtransformation_context,\n\u001b[1;32m   1099\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:945\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_data\u001b[0;34m(self, feature_view_obj, start_time, end_time, training_dataset_version, transformation_functions, read_options, spine, primary_keys, event_time, inference_helper_columns, dataframe_type, transformed, transformation_context)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_time:\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_eventtimes_from_query(feature_view_obj\u001b[38;5;241m.\u001b[39mquery)\n\u001b[1;32m    934\u001b[0m feature_dataframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_query(\n\u001b[1;32m    935\u001b[0m     feature_view_obj,\n\u001b[1;32m    936\u001b[0m     start_time,\n\u001b[1;32m    937\u001b[0m     end_time,\n\u001b[1;32m    938\u001b[0m     with_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    939\u001b[0m     primary_keys\u001b[38;5;241m=\u001b[39mprimary_keys,\n\u001b[1;32m    940\u001b[0m     event_time\u001b[38;5;241m=\u001b[39mevent_time,\n\u001b[1;32m    941\u001b[0m     inference_helper_columns\u001b[38;5;241m=\u001b[39minference_helper_columns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformed,\n\u001b[1;32m    942\u001b[0m     training_helper_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    943\u001b[0m     training_dataset_version\u001b[38;5;241m=\u001b[39mtraining_dataset_version,\n\u001b[1;32m    944\u001b[0m     spine\u001b[38;5;241m=\u001b[39mspine,\n\u001b[0;32m--> 945\u001b[0m )\u001b[38;5;241m.\u001b[39mread(read_options\u001b[38;5;241m=\u001b[39mread_options, dataframe_type\u001b[38;5;241m=\u001b[39mdataframe_type)\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformation_functions \u001b[38;5;129;01mand\u001b[39;00m transformed:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39m_apply_transformation_function(\n\u001b[1;32m    948\u001b[0m         transformation_functions,\n\u001b[1;32m    949\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mfeature_dataframe,\n\u001b[1;32m    950\u001b[0m         transformation_context\u001b[38;5;241m=\u001b[39mtransformation_context,\n\u001b[1;32m    951\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/hsfs/constructor/query.py:206\u001b[0m, in \u001b[0;36mQuery.read\u001b[0;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoins) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m schema]:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         )\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39msql(\n\u001b[1;32m    207\u001b[0m     sql_query,\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_store_name,\n\u001b[1;32m    209\u001b[0m     online_conn,\n\u001b[1;32m    210\u001b[0m     dataframe_type,\n\u001b[1;32m    211\u001b[0m     read_options,\n\u001b[1;32m    212\u001b[0m     schema,\n\u001b[1;32m    213\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/hsfs/engine/python.py:146\u001b[0m, in \u001b[0;36mEngine.sql\u001b[0;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     sql_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     schema: Optional[List[feature\u001b[38;5;241m.\u001b[39mFeature]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    144\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pl\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_offline(\n\u001b[1;32m    147\u001b[0m             sql_query,\n\u001b[1;32m    148\u001b[0m             dataframe_type,\n\u001b[1;32m    149\u001b[0m             schema,\n\u001b[1;32m    150\u001b[0m             arrow_flight_config\u001b[38;5;241m=\u001b[39mread_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow_flight_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m read_options\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdbc(\n\u001b[1;32m    156\u001b[0m             sql_query, online_conn, dataframe_type, read_options, schema\n\u001b[1;32m    157\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/hsfs/engine/python.py:197\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[0;34m(self, sql_query, dataframe_type, schema, arrow_flight_config)\u001b[0m\n\u001b[1;32m    189\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mrun_with_loading_animation(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data from Hopsworks, using Hopsworks Feature Query Service\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    191\u001b[0m         arrow_flight_client\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mread_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         dataframe_type,\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data with Hive is not supported when using hopsworks client version >= 4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema:\n\u001b[1;32m    201\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m Engine\u001b[38;5;241m.\u001b[39mcast_columns(result_df, schema)\n",
      "\u001b[0;31mValueError\u001b[0m: Reading data with Hive is not supported when using hopsworks client version >= 4.0"
     ]
    }
   ],
   "source": [
    "from src.inference import get_feature_store\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import src.config as config\n",
    "\n",
    "# Disable Arrow Flight to force Spark execution\n",
    "import os\n",
    "os.environ[\"HOPSWORKS_DISABLE_ARROW_FLIGHT\"] = \"true\"\n",
    "\n",
    "# Get the current UTC time  \n",
    "current_date = pd.Timestamp.now(tz='Etc/UTC')\n",
    "feature_store = get_feature_store()\n",
    "\n",
    "# Define the time window for fetching data\n",
    "fetch_data_to = current_date - timedelta(hours=1)\n",
    "fetch_data_from = current_date - timedelta(days=29)\n",
    "print(f\"Fetching data from {fetch_data_from} to {fetch_data_to}\")\n",
    "\n",
    "# Retrieve the feature view using your config values\n",
    "feature_view = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_NAME, version=config.FEATURE_VIEW_VERSION\n",
    ")\n",
    "\n",
    "# Now, fetch the batch data (this should force Spark because Arrow Flight is disabled)\n",
    "ts_data = feature_view.get_batch_data(\n",
    "    start_time=(fetch_data_from - timedelta(days=1)),\n",
    "    end_time=(fetch_data_to + timedelta(days=1)),\n",
    "    read_options={\"use_spark\": True, \"timeout\": 60000}  # timeout in ms (60 sec)\n",
    ")\n",
    "\n",
    "# Filter to the desired time window\n",
    "ts_data = ts_data[ts_data.pickup_hour.between(fetch_data_from, fetch_data_to)]\n",
    "ts_data.sort_values(by=[\"pickup_location_id\", \"pickup_hour\"], inplace=True)\n",
    "ts_data = ts_data.reset_index(drop=True)\n",
    "ts_data[\"pickup_hour\"] = ts_data[\"pickup_hour\"].dt.tz_localize(None)\n",
    "\n",
    "print(\"Batch data fetched successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b171dd4-628a-4c46-af00-92cee476f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 07:20:26,985 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-04 07:20:26,987 INFO: Initializing external client\n",
      "2025-03-04 07:20:26,988 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-04 07:20:27,673 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215646\n"
     ]
    }
   ],
   "source": [
    "from src.inference import get_feature_store\n",
    "\n",
    "feature_group = get_feature_store().get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_MODEL_PREDICTION,\n",
    "    version=1,\n",
    "    description=\"Predictions from LGBM Model\",\n",
    "    primary_key=[\"pickup_location_id\", \"pickup_hour\"],\n",
    "    event_time=\"pickup_hour\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ba8f0db-022e-4f8a-ac34-6fd8bb095b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_group\u001b[38;5;241m.\u001b[39minsert(predictions, write_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_for_job\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "feature_group.insert(predictions, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64b571-16f8-406f-9e44-dac714c87ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
